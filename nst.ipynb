{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1141,
   "id": "5cda3c37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kiranmanicka/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Users/kiranmanicka/Library/Python/3.9/lib/python/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VGG(\n",
      "  (features): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace=True)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (8): ReLU(inplace=True)\n",
      "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (11): ReLU(inplace=True)\n",
      "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (13): ReLU(inplace=True)\n",
      "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (15): ReLU(inplace=True)\n",
      "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (17): ReLU(inplace=True)\n",
      "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (20): ReLU(inplace=True)\n",
      "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (22): ReLU(inplace=True)\n",
      "    (23): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (24): ReLU(inplace=True)\n",
      "    (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (26): ReLU(inplace=True)\n",
      "    (27): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (29): ReLU(inplace=True)\n",
      "    (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (31): ReLU(inplace=True)\n",
      "    (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (33): ReLU(inplace=True)\n",
      "    (34): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (35): ReLU(inplace=True)\n",
      "    (36): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace=True)\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (4): ReLU(inplace=True)\n",
      "    (5): Dropout(p=0.5, inplace=False)\n",
      "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "model = models.vgg19(pretrained=True)\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "id": "9009bb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import PIL\n",
    "from PIL import Image \n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "id": "de675f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Normalization(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Normalization, self).__init__()\n",
    "        # .view the mean and std to make them [C x 1 x 1] so that they can\n",
    "        # directly work with image Tensor of shape [B x C x H x W].\n",
    "        # B is batch size. C is number of channels. H is height and W is width.\n",
    "        self.mean = torch.tensor([0.485, 0.456, 0.406]).view(-1,1,1)\n",
    "        self.std = torch.tensor([0.229, 0.224, 0.225]).view(-1,1,1)\n",
    "\n",
    "    def forward(self, img):\n",
    "        # normalize ``img``\n",
    "        return (img - self.mean) / self.std\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class mod_vgg(nn.Module):\n",
    "    def __init__(self,model,index):\n",
    "        super().__init__()\n",
    "        l=[]\n",
    "        i=0\n",
    "        for r in model.children():\n",
    "            if i==index:\n",
    "                l.append(nn.Flatten(start_dim=0, end_dim=-1))\n",
    "            l.append(r)\n",
    "            i+=1\n",
    "        self.layers = nn.ModuleList(l)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        # ModuleList can act as an iterable, or be indexed using ints\n",
    "        for r  in self.layers:\n",
    "            x=r(x)\n",
    "            print(x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "id": "5f115cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.9882, 0.9922, 0.9922,  ..., 0.9961, 0.9961, 0.9961],\n",
       "         [0.9686, 0.9725, 0.9686,  ..., 0.9843, 0.9882, 0.9882],\n",
       "         [0.9647, 0.9333, 0.8784,  ..., 0.9686, 0.9765, 0.9686],\n",
       "         ...,\n",
       "         [0.9059, 0.3843, 0.2431,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9608, 0.8941, 0.8471,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9882, 0.9882, 0.9843,  ..., 0.9922, 0.9922, 0.9961]],\n",
       "\n",
       "        [[0.9882, 0.9882, 0.9922,  ..., 0.9922, 0.9961, 0.9961],\n",
       "         [0.9725, 0.9804, 0.9765,  ..., 0.9843, 0.9882, 0.9882],\n",
       "         [0.9765, 0.9490, 0.8980,  ..., 0.9725, 0.9765, 0.9686],\n",
       "         ...,\n",
       "         [0.8980, 0.3765, 0.2392,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9569, 0.8902, 0.8471,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9882, 0.9882, 0.9843,  ..., 0.9922, 0.9922, 0.9961]],\n",
       "\n",
       "        [[0.9882, 0.9843, 0.9765,  ..., 0.9922, 0.9961, 0.9961],\n",
       "         [0.9725, 0.9765, 0.9765,  ..., 0.9843, 0.9882, 0.9882],\n",
       "         [0.9725, 0.9569, 0.9176,  ..., 0.9725, 0.9765, 0.9686],\n",
       "         ...,\n",
       "         [0.8902, 0.3686, 0.2275,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9529, 0.8863, 0.8431,  ..., 0.9922, 0.9922, 0.9961],\n",
       "         [0.9882, 0.9882, 0.9843,  ..., 0.9922, 0.9922, 0.9961]]])"
      ]
     },
     "execution_count": 1144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content_directory='content'\n",
    "style_directory='style'\n",
    "\n",
    "content_image = Image.open(f'{content_directory}/lion.jpeg') \n",
    "style_image=Image.open(f'{style_directory}/wave.jpeg')\n",
    "\n",
    "\n",
    "# Define a transform to convert PIL  \n",
    "# image to a Torch tensor \n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "]) \n",
    "  \n",
    "# transform = transforms.PILToTensor() \n",
    "# Convert the PIL image to Torch tensor \n",
    "content_tensor = transform(content_image)\n",
    "style_tensor=transform(style_image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "id": "0ce7780d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "print(content_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1146,
   "id": "45bd155c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.children at 0x3ec2efc10>"
      ]
     },
     "execution_count": 1146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.children()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1147,
   "id": "354b307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_model=mod_vgg(model,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b6c7a3",
   "metadata": {},
   "source": [
    "Figure out how to grab high level activations and gram matrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "id": "9e4a275e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(24032.2539)\n"
     ]
    }
   ],
   "source": [
    "print(torch.sum(content_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "id": "a2d412f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_loss_index=8\n",
    "style_loss_indices=[1,3,6,8,11]\n",
    "\n",
    "\n",
    "temp_layers=list(list(list(mod_model.children())[0].children())[0])\n",
    "temp_layers.insert(0,Normalization())\n",
    "\n",
    "\n",
    "def get_activations(image):\n",
    "    base_content_activation=None\n",
    "    base_style_activation=[]\n",
    "    for r in range(len(temp_layers)):\n",
    "        image=temp_layers[r](image)\n",
    "        if r==content_loss_index:\n",
    "            base_content_activation=image.clone()\n",
    "        if r in style_loss_indices:\n",
    "            base_style_activation.append(image.clone())\n",
    "            if style_loss_indices[-1]==r:\n",
    "                break\n",
    "    return base_content_activation,base_style_activation\n",
    "\n",
    "model.requires_grad_(False)\n",
    "for r in temp_layers:\n",
    "    r.requires_grad_(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "id": "ad1382f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[ 0.0516,  0.0004,  0.0712,  ..., -0.0217, -0.0094,  0.0009],\n",
       "         [ 0.0004,  0.0991, -0.0225,  ..., -0.0075, -0.0130,  0.0164],\n",
       "         [ 0.0712, -0.0225,  0.1244,  ..., -0.0500, -0.0142, -0.0076],\n",
       "         ...,\n",
       "         [-0.0217, -0.0075, -0.0500,  ...,  0.0330,  0.0085,  0.0048],\n",
       "         [-0.0094, -0.0130, -0.0142,  ...,  0.0085,  0.0115, -0.0037],\n",
       "         [ 0.0009,  0.0164, -0.0076,  ...,  0.0048, -0.0037,  0.0254]]),\n",
       " tensor([[ 0.2368,  0.0605,  0.0519,  ...,  0.0856,  0.1101, -0.0101],\n",
       "         [ 0.0605,  0.0705,  0.0293,  ...,  0.0008,  0.0915,  0.0150],\n",
       "         [ 0.0519,  0.0293,  0.0786,  ...,  0.0250,  0.0962, -0.0064],\n",
       "         ...,\n",
       "         [ 0.0856,  0.0008,  0.0250,  ...,  0.1292,  0.0239, -0.0250],\n",
       "         [ 0.1101,  0.0915,  0.0962,  ...,  0.0239,  0.2624,  0.0453],\n",
       "         [-0.0101,  0.0150, -0.0064,  ..., -0.0250,  0.0453,  0.2592]]),\n",
       " tensor([[ 0.7251,  0.0134,  0.1119,  ..., -0.2157,  0.2948,  0.2464],\n",
       "         [ 0.0134,  0.2292,  0.0048,  ...,  0.0141,  0.0871,  0.0191],\n",
       "         [ 0.1119,  0.0048,  0.2154,  ...,  0.0090, -0.2536, -0.0441],\n",
       "         ...,\n",
       "         [-0.2157,  0.0141,  0.0090,  ...,  0.3075, -0.2310, -0.1673],\n",
       "         [ 0.2948,  0.0871, -0.2536,  ..., -0.2310,  1.5013,  0.4696],\n",
       "         [ 0.2464,  0.0191, -0.0441,  ..., -0.1673,  0.4696,  0.3560]]),\n",
       " tensor([[ 4.3364e-01,  1.3558e-02, -1.4584e-01,  ..., -7.3621e-03,\n",
       "          -2.9516e-02,  1.8347e-02],\n",
       "         [ 1.3558e-02,  7.7327e-01,  4.8108e-02,  ..., -1.0324e-01,\n",
       "          -1.4424e-01, -2.5776e-03],\n",
       "         [-1.4584e-01,  4.8108e-02,  3.2319e-01,  ...,  5.3660e-03,\n",
       "          -1.2052e-01,  8.0744e-05],\n",
       "         ...,\n",
       "         [-7.3621e-03, -1.0324e-01,  5.3660e-03,  ...,  4.7124e-01,\n",
       "          -1.2119e-01,  3.6035e-02],\n",
       "         [-2.9516e-02, -1.4424e-01, -1.2052e-01,  ..., -1.2119e-01,\n",
       "           5.3217e-01, -2.0375e-02],\n",
       "         [ 1.8347e-02, -2.5776e-03,  8.0744e-05,  ...,  3.6035e-02,\n",
       "          -2.0375e-02,  2.2252e-01]]),\n",
       " tensor([[ 0.2522,  0.0359, -0.0530,  ..., -0.0203,  0.0314, -0.0305],\n",
       "         [ 0.0359,  0.5882,  0.1933,  ...,  0.1831, -0.1270,  0.2932],\n",
       "         [-0.0530,  0.1933,  0.3273,  ...,  0.1357, -0.0531,  0.1774],\n",
       "         ...,\n",
       "         [-0.0203,  0.1831,  0.1357,  ...,  0.8171, -0.0285,  0.2425],\n",
       "         [ 0.0314, -0.1270, -0.0531,  ..., -0.0285,  0.3337, -0.1036],\n",
       "         [-0.0305,  0.2932,  0.1774,  ...,  0.2425, -0.1036,  0.6561]])]"
      ]
     },
     "execution_count": 1150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_content,_=get_activations(content_tensor)\n",
    "_,base_style=get_activations(style_tensor)\n",
    "\n",
    "def calc_gram_matrix(feature_maps):\n",
    "    a,b,c=feature_maps.size()\n",
    "    features = feature_maps.view(a, b*c)\n",
    "    G = torch.mm(features, features.t())\n",
    "    #return G\n",
    "    return G.div(a * b * c)\n",
    "    \n",
    "#     return tensor\n",
    "\n",
    "base_gram_matrices=[calc_gram_matrix(r) for r in base_style]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2553cc",
   "metadata": {},
   "source": [
    "Construct loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "id": "b0446823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#detaching the base tensors\n",
    "base_content=base_content.detach()\n",
    "for r in range(len(base_style)):\n",
    "    base_style[r]=base_style[r].detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "id": "08070b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss function\n",
    "def overall_loss(img,content, style,base_content,base_style,version='both',variation=True):\n",
    "    def total_variation_loss(img, weight):\n",
    "        c_img, h_img, w_img = img.size()\n",
    "        tv_h = torch.pow(img[:,1:,:]-img[:,:-1,:], 2).sum()\n",
    "        tv_w = torch.pow(img[:,:,1:]-img[:,:,:-1], 2).sum()\n",
    "        return weight*(tv_h+tv_w)/(c_img*h_img*w_img)\n",
    "    if variation:\n",
    "        variation_loss=total_variation_loss(img,1000000)\n",
    "    else:\n",
    "        torch.tensor(0)\n",
    "    style_loss=nn.MSELoss()\n",
    "    content_loss=nn.MSELoss()\n",
    "    alpha=1\n",
    "    beta=100000\n",
    "    style_loss_tensor=torch.tensor(0)\n",
    "    content_loss_tensor=torch.tensor(0)\n",
    "    for r in range(len(base_style)):\n",
    "        style_loss_tensor=style_loss_tensor+ style_loss(calc_gram_matrix(style[r]),base_gram_matrices[r])\n",
    "    \n",
    "    content_loss_tensor=content_loss(content,base_content)\n",
    "    if version=='style':\n",
    "        return style_loss_tensor+variation_loss\n",
    "    if version=='content':\n",
    "        return content_loss_tensor+variation_loss\n",
    "    \n",
    "    return alpha*content_loss_tensor + beta * style_loss_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed97e53a",
   "metadata": {},
   "source": [
    "Gradient Descent Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "id": "6abe2908",
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "id": "ab3f7156",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\n",
      "[20]\n",
      "[40]\n",
      "[60]\n",
      "[80]\n",
      "[100]\n",
      "[120]\n",
      "[140]\n",
      "[160]\n",
      "[180]\n",
      "[200]\n",
      "[220]\n",
      "[240]\n",
      "[260]\n",
      "[280]\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "generated_image=content_tensor.clone()\n",
    "# generated_image = generated_image.to(torch.float)\n",
    "generated_image.requires_grad=True\n",
    "loss_list=[100,90]\n",
    "\n",
    "LBFGS_optimizer=optim.LBFGS([generated_image])\n",
    "optimizer=optim.SGD([generated_image],lr=.1)\n",
    "\n",
    "test_clone=generated_image.clone().detach()\n",
    "threshold=0\n",
    "i=[0]\n",
    "while i[0]<iterations:\n",
    "    # making predictions with forward pass\n",
    "    print(i)\n",
    "    def closure():\n",
    "        with torch.no_grad():\n",
    "            generated_image.clamp_(0, 1)\n",
    "        i[0]+=1\n",
    "        content,style=get_activations(generated_image)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        computed_loss=overall_loss(generated_image,content,style,base_content,base_style,\"both\",variation=\"False\")\n",
    "        LBFGS_optimizer.zero_grad()\n",
    "        # storing the calculated loss in a list\n",
    "        loss_list.append(computed_loss.item())\n",
    "       \n",
    "        computed_loss.backward()\n",
    "        return computed_loss\n",
    "    # updateing the parameters after each iteration\n",
    "    LBFGS_optimizer.step(closure)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1155,
   "id": "6569ea76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.6145, 0.6231, 0.7462,  ..., 0.3572, 0.1931, 0.1404],\n",
      "         [0.6730, 0.4451, 0.3993,  ..., 0.5510, 0.4929, 0.5342],\n",
      "         [0.5089, 0.5272, 0.4173,  ..., 0.5556, 0.5712, 0.5636],\n",
      "         ...,\n",
      "         [0.8573, 1.0026, 0.4742,  ..., 0.7108, 0.6983, 0.5989],\n",
      "         [0.7829, 0.9751, 0.9938,  ..., 0.7810, 0.6692, 0.6075],\n",
      "         [0.9225, 0.7345, 0.8518,  ..., 0.8949, 0.7889, 0.6343]],\n",
      "\n",
      "        [[0.4075, 0.5868, 0.8685,  ..., 0.3734, 0.3525, 0.2081],\n",
      "         [0.5897, 0.8139, 0.7864,  ..., 0.7420, 0.7712, 0.7654],\n",
      "         [0.6690, 0.6813, 0.7531,  ..., 0.6580, 0.6364, 0.5689],\n",
      "         ...,\n",
      "         [0.8175, 1.0007, 0.6955,  ..., 0.8735, 0.6842, 0.7278],\n",
      "         [0.5757, 0.9939, 0.9869,  ..., 0.9134, 0.6842, 0.6702],\n",
      "         [0.5045, 0.7174, 0.9671,  ..., 0.9908, 0.6384, 0.5971]],\n",
      "\n",
      "        [[0.5351, 0.7084, 0.6970,  ..., 0.4783, 0.3357, 0.3484],\n",
      "         [0.8030, 0.7023, 0.6863,  ..., 0.7174, 0.6495, 0.6373],\n",
      "         [0.8450, 0.7559, 0.8462,  ..., 0.8307, 0.7674, 0.7273],\n",
      "         ...,\n",
      "         [0.9961, 0.9994, 0.6994,  ..., 0.9915, 0.6617, 0.8398],\n",
      "         [0.8349, 0.8884, 0.9765,  ..., 1.0012, 0.6362, 0.6183],\n",
      "         [0.8366, 0.9525, 1.0008,  ..., 1.0013, 0.6293, 0.5998]]],\n",
      "       requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "invTrans2 = transforms.Compose([ \n",
    "                                transforms.ToPILImage()\n",
    "                                \n",
    "                               ])\n",
    "print(generated_image)\n",
    "with torch.no_grad():\n",
    "    generated_image.clamp_(0, 1)\n",
    "inversed_image=invTrans2(generated_image)\n",
    "inversed_image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19805269",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3a057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7c76de4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
